# DUFL2025 作文切题度评分与相关性评论生成系统

## 项目概述

本项目旨在为 DUFL2025 (NLPCC2025 Evaluation of Essay On-Topic Graded Comments - EOTGC) 评测任务提供解决方案。项目包含两个主要部分：

1.  **Track 1**: 对中小学生作文进行切题度自动评分，输出五个等级（优秀、较好、一般、合格、不合格）。
2.  **Track 2**: 基于 Track 1 的评分结果，自动生成仅聚焦于“切题度”和“中心思想”的相关性评论，并严格控制长度在 120-180 字符之间。

本项目利用大型语言模型（LLM）的多智能体协作框架，结合辅助的 BERT 模型和精心设计的处理流程，旨在实现准确、可靠的作文自动化评估与评论生成。

## 目录结构
.
├── DUFL2025_Method_Report（最高版本）.md # 详细的方法报告（中英文）
├── DUFL2025_track1.json # Track 1 的最终输出结果（切题度分类）
├── DUFL2025_track2.json # Track 2 的最终输出结果（相关性评论）
├── Task Guideline.md # 官方评测任务指南
├── result.xlsx # 测试的结果汇总分析表格
├── samples.json # 用于 Few-Shot 引导的样本数据
├── test_data.json # 评测用的测试集作文数据
├── track1.ipynb # 实现 Track 1 解决方案的 Jupyter Notebook
└── track2.ipynb # 实现 Track 2 解决方案的 Jupyter Notebook

## 方法说明

更详细的方法论、技术细节和实现请参见 `DUFL2025_Method_Report（最高版本）.md` 文件。

### Track 1: 作文切题度自动评分

本赛道采用了一种**混合架构**，结合了**基于 LLM 的多智能体协作系统**和一个**辅助性的 BERT 分类模块**，旨在利用测试数据自身的特点进行优化。

1.  **LLM 多智能体分析框架**:
    *   使用 `deepseekv3` 模型作为基础 LLM。
    *   构建了一个包含 7 个专门智能体（Agent 0-6）的级联处理流程，通过精心设计的提示工程引导，依次进行作文类型识别、题目要求解析、文章主旨提取、内容-任务符合度校验、素材-主题契合度审核、综合裁决（输出初步分类）和分类结果提取。
    *   部分智能体（Agent 4, 5）利用 `samples.json` 中的样本进行 Few-Shot 引导。
2.  **BERT 辅助分类模块**:
    *   使用 `bert-base-chinese` 作为预训练模型，构建了一个增强型 BERT 架构 (`EnhancedBertEssayModel`)，包含多头自注意力、多编码头、特征融合、BiLSTM 等。
    *   在**无监督**模式下，利用测试集数据 (`test_data.json`) 和对比损失函数（InfoNCE 风格）对 BERT 模型进行训练，使其更好地提取作文语义特征以区分切题度。训练好的模型保存为 `bert_essay_model.pt`。
    *   训练好的 BERT 模型为每篇作文提供一个独立的切题度预测分类。
3.  **混合决策与冲突仲裁**:
    *   主要以 LLM 多智能体框架的输出作为判定依据。
    *   当 LLM 框架与 BERT 模块预测不一致时，启动一个专门的 LLM **冲突仲裁智能体** (Arbiter Agent)，该智能体结合 `samples.json` 中的 Few-Shot 样本进行最终裁决。
    *   根据一致性、仲裁结果或在失败情况下的回退逻辑确定最终分类。
    *   最终结果经过 ID 格式化后保存到 `DUFL2025_track1.json`。

### Track 2: 相关性评论自动生成

本赛道设计了一个**多阶段 LLM 智能体流水线**，旨在生成**严格聚焦于切题度和中心思想**，且长度**精确控制在 120-180 字符之间**的自动化评语。

1.  **系统输入**: 依赖 `test_data.json` 中的作文数据，并关键性地**利用 Track 1 输出的最终切题度分类结果** (`DUFL2025_track1.json`) 作为指导。同时，也使用 `samples.json` 进行 Few-Shot 引导。
2.  **LLM 评论生成流水线**:
    *   使用 `qwen-max` 模型作为基础 LLM。
    *   包含一系列智能体（Agent 0-2, A-D, 4, 5），依次进行作文类型识别、题目要求分析、主旨提取、年级适配性评估、写作技巧分析（对切题影响）、素材选择评估（对切题影响）、结构逻辑分析（对切题影响）。
    *   **初步评语草稿生成智能体 (Agent 4)**：结合所有分析报告和 Track 1 分类结果，生成初步评语草稿，提示被严格约束以聚焦切题性、中心思想、建议方向（仅限切题提升）和参考样本，目标长度为 190-220 字符。
    *   **合规性检查与精炼智能体 (Agent 5)**：对草稿进行最终审核，执行严格的检查和优化：
        *   **内容约束验证**: 强制聚焦“切题度/中心思想”，移除无关评价。
        *   **格式规范化**: 输出纯文本。
        *   **长度约束执行**: 严格确保最终字符数在 **120-180 个字符之间**（通过 `clean_comment` 函数和流程逻辑强制实现）。
        *   **分类一致性复核**。
        *   **语言专业性润色**。
        *   **总结句规范**: 对“优秀”至“合格”的评语，确保最后一句是规范总结句（特定开头+重申切题表现，不含建议）。
3.  **缓存与容错**:
    *   使用 `processed_comments_cache_track2.json` 缓存已成功生成的、符合长度要求的评论。
    *   包含重试机制，若生成失败或长度不合规，会尝试重新执行整个 Agent 链。
    *   若多次重试后仍失败，则根据 Track 1 分类生成模板化的**备用评语**，并强制调整长度至 120-180 字符。
4.  **输出格式化**: 将所有评论与其在 `test_data.json` 中对应的索引（即 Track 1 ID）匹配，保存到 `DUFL2025_track2.json`，确保 ID 与 Track 1 输出一致。

## 文件说明

*   `DUFL2025_Method_Report（最高版本）.md`: 详细描述了两个赛道所采用的技术方法、模型架构、处理流程和实现细节的报告文档（包含中英文版本）。
*   `DUFL2025_track1.json`: **赛道一的最终输出文件**。包含对测试集中每篇作文的切题度分类结果（优秀、较好、一般、合格、不合格），格式为 `[{"id": 0, "classification": "..."}, ...]`。ID 从 0 开始，与 `test_data.json` 中的索引对应。
*   `DUFL2025_track2.json`: **赛道二的最终输出文件**。包含对测试集中每篇作文生成的、符合要求（聚焦切题度与中心思想，长度 120-180 字符）的相关性评论，格式为 `[{"id": 0, "comment": "..."}, ...]`。ID 从 0 开始，与 `test_data.json` 中的索引对应。
*   `Task Guideline.md`: 官方发布的评测任务指南，包含任务背景、描述、数据格式、评估标准等信息。
*   `result.xlsx`: Excel 文件，根据文件名推测可能包含了运行过程中的一些中间结果、详细分析数据或最终结果的另一种格式化表示（具体用途需查看文件内容或代码）。
*   `samples.json`: 样本数据文件，包含带有标准分类和评论的作文样例，用于在 LLM 推理过程中提供 Few-Shot 示例，以引导模型更好地理解任务要求和输出格式。
*   `test_data.json`: 官方提供的测试数据集，包含需要进行评分和评论生成的作文原文及其相关信息（如年级、题目要求、标题、内容），是两个赛道处理的主要输入。
*   `track1.ipynb`: 实现了赛道一（作文切题度自动评分）解决方案的 Jupyter Notebook 代码文件。包含了数据处理、模型调用（LLM API、BERT）、混合决策、结果生成等逻辑。
*   `track2.ipynb`: 实现了赛道二（相关性评论自动生成）解决方案的 Jupyter Notebook 代码文件。包含了数据处理、依赖 Track 1 结果、LLM 智能体流水线调用、评论合规性检查与长度控制、结果生成等逻辑。

## 注意事项

*   根据 `Task Guideline.md` 中的声明，竞赛期间获得的数据**严格限制**在此次竞赛范围内使用，禁止用于商业目的。如需用于研究，请联系组织者。

## 特别感谢

*   Dong Haoxiang (East China Normal University)
